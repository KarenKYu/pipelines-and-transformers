{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/pipelines-and-transformers/master/nyc_hs_sat.csv\"\n",
    "hs_df = pd.read_csv(url, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 356 entries, 0 to 355\n",
      "Data columns (total 11 columns):\n",
      "dbn                    356 non-null object\n",
      "name                   356 non-null object\n",
      "num_test_takers        327 non-null float64\n",
      "reading_avg            327 non-null float64\n",
      "math_avg               327 non-null float64\n",
      "writing_score          327 non-null float64\n",
      "boro                   356 non-null object\n",
      "total_students         356 non-null int64\n",
      "graduation_rate        351 non-null float64\n",
      "attendance_rate        356 non-null float64\n",
      "college_career_rate    351 non-null float64\n",
      "dtypes: float64(7), int64(1), object(3)\n",
      "memory usage: 33.4+ KB\n"
     ]
    }
   ],
   "source": [
    "hs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this time, let's use transformers with our pipelines.  Pipelines are useful because they condense the transformations that we make to our data.  And they allow us to apply the same transformations to new arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say that we want to replace our null values with the mean and then translate our values into the respective z-scores.\n",
    "\n",
    "We can do so with a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the `Pipeline` class from the `pipeline` module.  \n",
    "\n",
    "Then we initialize a new `Pipeline` instance, specifying the transformations that we would like to make to our data.\n",
    "\n",
    "> Here we'll just focus on transforming one column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer()),\n",
    "    ('standardize', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass `Pipeline` the keyword argument `steps`, which takes a list of steps.  Each step is represented as a tuple: \n",
    "\n",
    "```python\n",
    "('impute', SimpleImputer())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the tuple is a name that we assign the step, and then second element is an instance of the transformer that we would like to apply.\n",
    "\n",
    "> We can name the step whatever we prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize the pipeline, we can then examine the steps with the `named_steps` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impute': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='mean', verbose=0),\n",
       " 'standardize': StandardScaler(copy=True, with_mean=True, with_std=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this just returns a dictionary of our defined steps, with each key pointing to the respective transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we used the pipeline to define the tranformations we would like to apply, we can try it on some data.\n",
    "\n",
    "We do so using the same interface that we saw with transformers: fit to learn parameters from the data, and `transform` to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('impute',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('standardize',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graduation_rate = hs_df[['graduation_rate']]\n",
    "\n",
    "pipeline.fit(graduation_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_grad = pipeline.transform(graduation_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97597463],\n",
       "       [ 0.8043654 ],\n",
       "       [ 0.95272707],\n",
       "       [-0.38252795],\n",
       "       [ 1.32363125],\n",
       "       [ 1.47199292]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_grad[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now see that both transfomations were applied to our column above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw how we can use pipelines to help us with feature engineering.  Pipelines store a sequence of transformers and then call the transformers for us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer()),\n",
    "    ('standardize', StandardScaler()),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a pipeline by passing a list of steps to the `steps` argument.  Each step is a tuple, where the first element is the name that we assign to the step, and the second argument is an instance of the transformer that we wish to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipelines Kaggle Tutorial](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
